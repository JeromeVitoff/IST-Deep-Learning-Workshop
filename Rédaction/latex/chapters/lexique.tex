% !TEX root = manuscript.tex
\section*{Lexique}
\addcontentsline{toc}{section}{Lexique}
\begin{flushleft}
\begin{itemize}
\item \textbf{Batch Normalization:} Technique normalisant les activations d'une couche au sein d'un mini-batch pour stabiliser et accélérer l'entraînement des réseaux profonds.
\item \textbf{CNN (Convolutional Neural Network):} Réseau de neurones utilisant des opérations de convolution pour extraire des caractéristiques spatiales dans les données, particulièrement efficace pour l'analyse d'images.
\item \textbf{Convolution :} Opération mathématique appliquant des filtres à une matrice d'entrée pour détecter des motifs locaux.
\item \textbf{Dropout :} Technique de régularisation qui désactive aléatoirement certains neurones pendant l'entraînement pour réduire le surapprentissage.
\item \textbf{Early Stopping :} Méthode consistant à arrêter l'entraînement lorsque la performance sur l'ensemble de validation cesse de s'améliorer.
\item \textbf{Fonction d'activation} Fonction non linéaire appliquée à la sortie d'un neurone, comme ReLU (Rectified Linear Unit) ou softmax.
\item \textbf{Hyperparamètre :} Paramètre défini avant l'entraînement (taux d'apprentissage, nombre de couches, etc.) par opposition aux paramètres appris pendant l'entraînement.
\item \textbf{Matrice de confusion :} Tableau récapitulatif permettant de visualiser les performances d'un algorithme de classification, montrant les prédictions correctes et incorrectes pour chaque classe.
\item \textbf{MLP (Multi-Layer Perceptron) :} Réseau de neurones composé de couches de neurones entièrement connectées, sans notion de convolution ou de structure spatiale.
\item \textbf{Pooling :} Opération réduisant la dimension spatiale des données tout en préservant les caractéristiques importantes, généralement par max pooling (conservation de la valeur maximale).
\item \textbf{ResNet (Residual Network) :} Architecture introduisant des connexions résiduelles (skip connections) permettant d'entraîner efficacement des réseaux très profonds.
\item \textbf{Surapprentissage :} Phénomène où un modèle apprend trop parfaitement les données d'entraînement au détriment de sa capacité à généraliser sur de nouvelles données.
\item \textbf{Taux d'apprentissage :} Hyperparamètre contrôlant l'amplitude des ajustements des poids lors de l'entraînement.
\end{itemize}
\end{flushleft}